{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 11236914,
          "sourceType": "datasetVersion",
          "datasetId": 7019964
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports**"
      ],
      "metadata": {
        "id": "wOZR3oL_afHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "from torch.optim import Adam\n",
        "from torchvision.models import inception_v3\n",
        "from scipy.linalg import sqrtm\n",
        "import math\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-01T10:02:19.486991Z",
          "iopub.execute_input": "2025-04-01T10:02:19.487316Z",
          "iopub.status.idle": "2025-04-01T10:02:19.494421Z",
          "shell.execute_reply.started": "2025-04-01T10:02:19.487290Z",
          "shell.execute_reply": "2025-04-01T10:02:19.493678Z"
        },
        "id": "UkZAKDKwafHD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define constants**"
      ],
      "metadata": {
        "id": "SgApQlHJafHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define constants\n",
        "IMAGE_SIZE = 150\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100\n",
        "LEARNING_RATE = 2e-4\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "TIMESTEPS = 1000\n",
        "BETA_START = 1e-4\n",
        "BETA_END = 0.02\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-01T10:02:30.040711Z",
          "iopub.execute_input": "2025-04-01T10:02:30.040991Z",
          "iopub.status.idle": "2025-04-01T10:02:30.045504Z",
          "shell.execute_reply.started": "2025-04-01T10:02:30.040969Z",
          "shell.execute_reply": "2025-04-01T10:02:30.044668Z"
        },
        "id": "20Jh6x1MafHE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extract timestep embeddings**"
      ],
      "metadata": {
        "id": "eQICbwysafHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to extract timestep embeddings\n",
        "def extract(a, t, x_shape):\n",
        "    batch_size = t.shape[0]\n",
        "    # Make sure t is on the same device as a\n",
        "    out = a.gather(-1, t)\n",
        "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-01T10:02:30.702701Z",
          "iopub.execute_input": "2025-04-01T10:02:30.703085Z",
          "iopub.status.idle": "2025-04-01T10:02:30.708247Z",
          "shell.execute_reply.started": "2025-04-01T10:02:30.703052Z",
          "shell.execute_reply": "2025-04-01T10:02:30.707201Z"
        },
        "id": "a-5gmAmkafHE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Normalizing image**"
      ],
      "metadata": {
        "id": "NkHB-S1safHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_image(img_tensor):\n",
        "    \"\"\"Normalize image to [-1, 1] range with better handling of edge cases\"\"\"\n",
        "    min_val = img_tensor.min()\n",
        "    max_val = img_tensor.max()\n",
        "\n",
        "    # Check for division by zero or very small values\n",
        "    if max_val - min_val < 1e-5:\n",
        "        return torch.zeros_like(img_tensor)\n",
        "\n",
        "    # Normalize to [0, 1] then to [-1, 1]\n",
        "    normalized = (img_tensor - min_val) / (max_val - min_val)\n",
        "    return 2.0 * normalized - 1.0\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-01T10:02:31.317209Z",
          "iopub.execute_input": "2025-04-01T10:02:31.317551Z",
          "iopub.status.idle": "2025-04-01T10:02:31.322172Z",
          "shell.execute_reply.started": "2025-04-01T10:02:31.317522Z",
          "shell.execute_reply": "2025-04-01T10:02:31.321122Z"
        },
        "id": "_yzA_9xsafHE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Handling**"
      ],
      "metadata": {
        "id": "jgP4WYu_afHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class for loading .npy lensing images\n",
        "class LensingDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_paths = glob.glob(os.path.join(data_dir, \"*.npy\"))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data_path = self.data_paths[idx]\n",
        "        # Load and process the .npy file\n",
        "        img = np.load(data_path)\n",
        "        img = img[0]  # Get the first channel as shape is (1, 150, 150)\n",
        "\n",
        "        # Convert to tensor\n",
        "        img_tensor = torch.tensor(img, dtype=torch.float32)\n",
        "        img_tensor = img_tensor.unsqueeze(0)  # Add channel dimension [1, 150, 150]\n",
        "\n",
        "        # Apply transformations if any\n",
        "        if self.transform:\n",
        "            img_tensor = self.transform(img_tensor)\n",
        "\n",
        "        # Normalize to [-1, 1]\n",
        "        img_tensor = normalize_image(img_tensor)\n",
        "\n",
        "        return img_tensor\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-01T10:02:32.002869Z",
          "iopub.execute_input": "2025-04-01T10:02:32.003167Z",
          "iopub.status.idle": "2025-04-01T10:02:32.008590Z",
          "shell.execute_reply.started": "2025-04-01T10:02:32.003145Z",
          "shell.execute_reply": "2025-04-01T10:02:32.007855Z"
        },
        "id": "HDa4UvuVafHE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **U-Net Building Blocks**"
      ],
      "metadata": {
        "id": "fUGgUTHPafHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define U-Net building blocks\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, down=True, act=\"relu\", dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1) if down\n",
        "            else nn.ConvTranspose2d(in_channels, out_channels, 3, 1, 1),\n",
        "            nn.GroupNorm(1, out_channels),\n",
        "            nn.ReLU() if act == \"relu\" else nn.SiLU(),\n",
        "            nn.Dropout(dropout) if dropout > 0 else nn.Identity(),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
        "            nn.GroupNorm(1, out_channels),\n",
        "            nn.ReLU() if act == \"relu\" else nn.SiLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, channels, time_emb_dim=None):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n",
        "        self.norm1 = nn.GroupNorm(1, channels)\n",
        "        self.norm2 = nn.GroupNorm(1, channels)\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        if time_emb_dim is not None:\n",
        "            self.time_mlp = nn.Sequential(\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(time_emb_dim, channels)\n",
        "            )\n",
        "        else:\n",
        "            self.time_mlp = None\n",
        "\n",
        "    def forward(self, x, t=None):\n",
        "        h = self.act(self.norm1(self.conv1(x)))\n",
        "\n",
        "        if self.time_mlp is not None and t is not None:\n",
        "            time_emb = self.time_mlp(t)\n",
        "            h = h + time_emb.reshape(-1, time_emb.shape[1], 1, 1)\n",
        "\n",
        "        h = self.act(self.norm2(self.conv2(h)))\n",
        "        return h + x\n",
        "\n",
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, channels, size):\n",
        "        super(Attention, self).__init__()\n",
        "        self.channels = channels\n",
        "        self.size = size\n",
        "        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n",
        "        self.ln = nn.LayerNorm([channels])\n",
        "        self.ff_self = nn.Sequential(\n",
        "            nn.LayerNorm([channels]),\n",
        "            nn.Linear(channels, channels),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(channels, channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.channels, self.size * self.size).swapaxes(1, 2)\n",
        "        x_ln = self.ln(x)\n",
        "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
        "        attention_value = attention_value + x\n",
        "        attention_value = self.ff_self(attention_value) + attention_value\n",
        "        return attention_value.swapaxes(2, 1).view(-1, self.channels, self.size, self.size)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-01T10:02:32.643631Z",
          "iopub.execute_input": "2025-04-01T10:02:32.643927Z",
          "iopub.status.idle": "2025-04-01T10:02:32.657702Z",
          "shell.execute_reply.started": "2025-04-01T10:02:32.643903Z",
          "shell.execute_reply": "2025-04-01T10:02:32.656816Z"
        },
        "id": "WXFLVzKCafHE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **U-Net model**"
      ],
      "metadata": {
        "id": "nyoH1Sg-afHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# U-Net model for diffusion\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1, time_dim=256, features=64):\n",
        "        super().__init__()\n",
        "\n",
        "        # Time embedding\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPositionEmbeddings(time_dim),\n",
        "            nn.Linear(time_dim, time_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Initial conv\n",
        "        self.conv0 = nn.Conv2d(in_channels, features, kernel_size=3, padding=1)\n",
        "\n",
        "        # Down sampling\n",
        "        self.downs = nn.ModuleList([\n",
        "            Block(features, features, act=\"silu\"),\n",
        "            Block(features, features * 2, act=\"silu\"),\n",
        "            Block(features * 2, features * 2, act=\"silu\"),\n",
        "            Block(features * 2, features * 4, act=\"silu\"),\n",
        "            Block(features * 4, features * 4, act=\"silu\"),\n",
        "        ])\n",
        "\n",
        "        # Middle blocks with attention\n",
        "        self.mid_block1 = ResBlock(features * 4, time_dim)\n",
        "        self.mid_attn = Attention(features * 4, IMAGE_SIZE // 16)\n",
        "        self.mid_block2 = ResBlock(features * 4, time_dim)\n",
        "\n",
        "        # Up sampling with matching dimensions\n",
        "        self.ups = nn.ModuleList([\n",
        "            Block(features * 8, features * 4, down=False, act=\"silu\"),\n",
        "            Block(features * 6, features * 2, down=False, act=\"silu\"),\n",
        "            Block(features * 4, features * 2, down=False, act=\"silu\"),\n",
        "            Block(features * 3, features, down=False, act=\"silu\"),\n",
        "        ])\n",
        "\n",
        "        # Max pooling for downsampling\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # Upsampling\n",
        "        self.upsamples = nn.ModuleList([\n",
        "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
        "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
        "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
        "        ])\n",
        "\n",
        "        # Final conv\n",
        "        self.final = nn.Sequential(\n",
        "            nn.Conv2d(features * 2, features, kernel_size=3, padding=1),\n",
        "            nn.GroupNorm(1, features),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(features, out_channels, kernel_size=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        # Time embedding\n",
        "        t = self.time_mlp(t)\n",
        "\n",
        "        # Initial feature extraction\n",
        "        x = self.conv0(x)\n",
        "\n",
        "        # Store residuals for skip connections\n",
        "        residuals = []\n",
        "        residuals.append(x)\n",
        "\n",
        "        # Downsample and store intermediate outputs\n",
        "        downs_outputs = []\n",
        "        current = x\n",
        "        for i, down in enumerate(self.downs):\n",
        "            current = down(current)\n",
        "            if i < len(self.downs) - 1:  # Don't pool the last layer\n",
        "                residuals.append(current)\n",
        "                current = self.pool(current)\n",
        "            else:\n",
        "                downs_outputs.append(current)\n",
        "\n",
        "        # Middle with attention\n",
        "        x = self.mid_block1(downs_outputs[-1], t)\n",
        "        x = self.mid_attn(x)\n",
        "        x = self.mid_block2(x, t)\n",
        "\n",
        "        # Upsample with skip connections - Fixed dimensions\n",
        "        for i, up in enumerate(self.ups):\n",
        "            residual = residuals.pop() if residuals else None\n",
        "            # Make sure dimensions match before concatenation\n",
        "            if residual is not None:\n",
        "                if x.shape[2:] != residual.shape[2:]:\n",
        "                    x = F.interpolate(x, size=residual.shape[2:], mode=\"nearest\")\n",
        "                x = torch.cat((x, residual), dim=1)\n",
        "            x = up(x)\n",
        "            if i < len(self.ups) - 1 and i < len(self.upsamples):\n",
        "                x = self.upsamples[i](x)\n",
        "\n",
        "        # Final residual connection and output\n",
        "        last_residual = residuals.pop() if residuals else None\n",
        "        if last_residual is not None:\n",
        "            # Make sure dimensions match before concatenation\n",
        "            if x.shape[2:] != last_residual.shape[2:]:\n",
        "                x = F.interpolate(x, size=last_residual.shape[2:], mode=\"nearest\")\n",
        "            x = torch.cat((x, last_residual), dim=1)\n",
        "\n",
        "        return self.final(x)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-01T10:02:33.299964Z",
          "iopub.execute_input": "2025-04-01T10:02:33.300286Z",
          "iopub.status.idle": "2025-04-01T10:02:33.312173Z",
          "shell.execute_reply.started": "2025-04-01T10:02:33.300259Z",
          "shell.execute_reply": "2025-04-01T10:02:33.311375Z"
        },
        "id": "zQ1922k4afHF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Diffusion model**"
      ],
      "metadata": {
        "id": "a_QjZWbnafHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Diffusion model\n",
        "class DiffusionModel:\n",
        "    def __init__(self, noise_steps=TIMESTEPS, beta_start=BETA_START, beta_end=BETA_END, img_size=IMAGE_SIZE, device=DEVICE):\n",
        "        self.noise_steps = noise_steps\n",
        "        self.beta_start = beta_start\n",
        "        self.beta_end = beta_end\n",
        "        self.img_size = img_size\n",
        "        self.device = device\n",
        "\n",
        "        # Linear noise schedule\n",
        "        self.betas = torch.linspace(beta_start, beta_end, noise_steps).to(device)\n",
        "        self.alphas = 1. - self.betas\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
        "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "\n",
        "        # Diffusion process parameters\n",
        "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
        "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n",
        "        self.sqrt_recip_alphas = torch.sqrt(1.0 / self.alphas)\n",
        "        self.posterior_variance = self.betas * (1. - self.alphas_cumprod_prev) / (1. - self.alphas_cumprod)\n",
        "\n",
        "    def q_sample(self, x_0, t, noise=None):\n",
        "        \"\"\"Forward diffusion process: add noise to the image\"\"\"\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x_0)\n",
        "\n",
        "        sqrt_alphas_cumprod_t = extract(self.sqrt_alphas_cumprod, t, x_0.shape)\n",
        "        sqrt_one_minus_alphas_cumprod_t = extract(self.sqrt_one_minus_alphas_cumprod, t, x_0.shape)\n",
        "\n",
        "        return sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise, noise\n",
        "\n",
        "    def p_sample(self, model, x, t, t_index):\n",
        "        \"\"\"Sample from the learned model at step t\"\"\"\n",
        "        betas_t = extract(self.betas, t, x.shape)\n",
        "        sqrt_one_minus_alphas_cumprod_t = extract(self.sqrt_one_minus_alphas_cumprod, t, x.shape)\n",
        "        sqrt_recip_alphas_t = extract(self.sqrt_recip_alphas, t, x.shape)\n",
        "\n",
        "        # Predicted noise\n",
        "        model_output = model(x, t)\n",
        "\n",
        "        # No noise for t=0 step\n",
        "        model_mean = sqrt_recip_alphas_t * (x - betas_t * model_output / sqrt_one_minus_alphas_cumprod_t)\n",
        "\n",
        "        if t_index == 0:\n",
        "            return model_mean\n",
        "        else:\n",
        "            posterior_variance_t = extract(self.posterior_variance, t, x.shape)\n",
        "            noise = torch.randn_like(x)\n",
        "            return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def p_sample_loop(self, model, shape):\n",
        "        \"\"\"Generate image by iteratively denoising\"\"\"\n",
        "        device = next(model.parameters()).device\n",
        "        b = shape[0]\n",
        "        # Use improved initial noise distribution\n",
        "        img = torch.randn(shape, device=device)\n",
        "        img = torch.clamp(img, -2, 2)  # Truncate extreme values\n",
        "\n",
        "        for i in tqdm(reversed(range(0, self.noise_steps)), desc='Sampling', total=self.noise_steps):\n",
        "            t = torch.full((b,), i, device=device, dtype=torch.long)\n",
        "            img = self.p_sample(model, img, t, i)\n",
        "\n",
        "        return img\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, model, n_samples, channels=1):\n",
        "        \"\"\"Sample new images\"\"\"\n",
        "        return self.p_sample_loop(model, (n_samples, channels, self.img_size, self.img_size))\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-01T10:02:42.100808Z",
          "iopub.execute_input": "2025-04-01T10:02:42.101089Z",
          "iopub.status.idle": "2025-04-01T10:02:42.111042Z",
          "shell.execute_reply.started": "2025-04-01T10:02:42.101069Z",
          "shell.execute_reply": "2025-04-01T10:02:42.110210Z"
        },
        "id": "WwC65DXtafHF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Learning Rate Scheduler**"
      ],
      "metadata": {
        "id": "tM0jgojpafHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a learning rate scheduler\n",
        "def create_lr_scheduler(optimizer, total_epochs):\n",
        "    \"\"\"Create a learning rate scheduler that decays over time\"\"\"\n",
        "    return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_epochs)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-01T10:02:45.570842Z",
          "iopub.execute_input": "2025-04-01T10:02:45.571120Z",
          "iopub.status.idle": "2025-04-01T10:02:45.574982Z",
          "shell.execute_reply.started": "2025-04-01T10:02:45.571099Z",
          "shell.execute_reply": "2025-04-01T10:02:45.574289Z"
        },
        "id": "H1yLu5L4afHG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FID score calculation**"
      ],
      "metadata": {
        "id": "OjggUSGPafHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement FID score calculation\n",
        "def calculate_fid(real_images, fake_images, device=DEVICE):\n",
        "    \"\"\"Calculate FID score between two sets of images\"\"\"\n",
        "    # Load the pretrained model but don't require grad\n",
        "    model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Handle smaller batch sizes for memory constraints\n",
        "    batch_size = 8\n",
        "\n",
        "    # Resize images to inception input size\n",
        "    resize = transforms.Resize((299, 299))\n",
        "\n",
        "    # Function to get features\n",
        "    def get_features(images):\n",
        "        features = []\n",
        "        # Process in batches to avoid OOM\n",
        "        for i in range(0, len(images), batch_size):\n",
        "            batch = images[i:i+batch_size]\n",
        "            batch_features = []\n",
        "\n",
        "            for img in batch:\n",
        "                img = resize(img)\n",
        "                # Expand to 3 channels if grayscale\n",
        "                if img.shape[0] == 1:\n",
        "                    img = img.repeat(3, 1, 1)\n",
        "                # Move to range [0, 1] then normalize for Inception\n",
        "                img = (img + 1) / 2.0  # Convert from [-1, 1] to [0, 1]\n",
        "                img = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img)\n",
        "                img = img.unsqueeze(0).to(device)\n",
        "\n",
        "                try:\n",
        "                    with torch.no_grad():\n",
        "                        # Use the correct output from InceptionV3\n",
        "                        feat = model(img)\n",
        "                        if isinstance(feat, tuple):\n",
        "                            feat = feat[0]  # Get the logits, not the aux_logits\n",
        "                        feat = feat.squeeze()\n",
        "                    batch_features.append(feat.cpu().numpy())\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing image: {e}\")\n",
        "                    continue\n",
        "\n",
        "            if batch_features:\n",
        "                features.extend(batch_features)\n",
        "\n",
        "        if not features:\n",
        "            raise ValueError(\"No features were successfully extracted\")\n",
        "\n",
        "        return np.stack(features, axis=0)\n",
        "\n",
        "    try:\n",
        "        # Get activations\n",
        "        real_features = get_features(real_images)\n",
        "        fake_features = get_features(fake_images)\n",
        "\n",
        "        # Check feature dimensions\n",
        "        print(f\"Feature shapes - Real: {real_features.shape}, Fake: {fake_features.shape}\")\n",
        "\n",
        "        # Calculate mean and covariance\n",
        "        mu_real = np.mean(real_features, axis=0)\n",
        "        sigma_real = np.cov(real_features, rowvar=False)\n",
        "\n",
        "        mu_fake = np.mean(fake_features, axis=0)\n",
        "        sigma_fake = np.cov(fake_features, rowvar=False)\n",
        "\n",
        "        # Ensure matrices are proper shape\n",
        "        print(f\"Covariance shapes - Real: {sigma_real.shape}, Fake: {sigma_fake.shape}\")\n",
        "\n",
        "        # Calculate FID score\n",
        "        ssdiff = np.sum((mu_real - mu_fake) ** 2.0)\n",
        "\n",
        "        # Handle numerical stability in sqrtm\n",
        "        try:\n",
        "            # Ensure matrices are proper format for sqrtm\n",
        "            if sigma_real.shape[0] > 0 and sigma_fake.shape[0] > 0:\n",
        "                # Check if matrices are positive semi-definite\n",
        "                # Add a small epsilon to diagonal for numerical stability\n",
        "                epsilon = 1e-6\n",
        "                sigma_real += np.eye(sigma_real.shape[0]) * epsilon\n",
        "                sigma_fake += np.eye(sigma_fake.shape[0]) * epsilon\n",
        "\n",
        "                covmean = sqrtm(sigma_real.dot(sigma_fake))\n",
        "\n",
        "                # Check if covmean contains complex numbers\n",
        "                if np.iscomplexobj(covmean):\n",
        "                    covmean = covmean.real\n",
        "\n",
        "                fid = ssdiff + np.trace(sigma_real + sigma_fake - 2.0 * covmean)\n",
        "            else:\n",
        "                print(\"Error: Empty covariance matrices\")\n",
        "                fid = ssdiff\n",
        "        except Exception as e:\n",
        "            print(f\"Error in FID sqrtm calculation: {e}\")\n",
        "            # Fallback to just the mean squared difference\n",
        "            fid = ssdiff\n",
        "\n",
        "        return fid\n",
        "    except Exception as e:\n",
        "        print(f\"Error in FID calculation: {e}\")\n",
        "        return float('inf')  # Return infinity if calculation fails\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-01T10:02:46.427578Z",
          "iopub.execute_input": "2025-04-01T10:02:46.427881Z",
          "iopub.status.idle": "2025-04-01T10:02:46.438104Z",
          "shell.execute_reply.started": "2025-04-01T10:02:46.427859Z",
          "shell.execute_reply": "2025-04-01T10:02:46.437227Z"
        },
        "id": "L4fuUCFQafHG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "32h_tcYAafHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train(model, diffusion, dataloader, optimizer, device=DEVICE, epochs=EPOCHS):\n",
        "    model.to(device)\n",
        "\n",
        "    # Create directories for saving\n",
        "    os.makedirs(\"model_checkpoints\", exist_ok=True)\n",
        "    os.makedirs(\"generated_samples\", exist_ok=True)\n",
        "    os.makedirs(\"loss_plots\", exist_ok=True)\n",
        "\n",
        "    # Initialize learning rate scheduler\n",
        "    scheduler = create_lr_scheduler(optimizer, epochs)\n",
        "\n",
        "    # Keep track of losses for plotting\n",
        "    epoch_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        batch_losses = []\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        for i, images in enumerate(progress_bar):\n",
        "            images = images.to(device)\n",
        "            batch_size = images.shape[0]\n",
        "\n",
        "            # Sample random timesteps\n",
        "            t = torch.randint(0, diffusion.noise_steps, (batch_size,), device=device).long()\n",
        "\n",
        "            # Add noise to images\n",
        "            x_t, noise = diffusion.q_sample(images, t)\n",
        "\n",
        "            # Predict noise\n",
        "            predicted_noise = model(x_t, t)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = F.mse_loss(predicted_noise, noise)\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping for stability\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_loss = loss.item()\n",
        "            epoch_loss += batch_loss\n",
        "            batch_losses.append(batch_loss)\n",
        "\n",
        "            progress_bar.set_postfix({\"loss\": batch_loss})\n",
        "\n",
        "        # Step the learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "        epoch_losses.append(avg_loss)\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Average Loss: {avg_loss:.4f} - LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "        # Plot and save loss curve\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(epoch_losses, label='Training Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Training Loss Over Time')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig(f\"loss_plots/loss_curve_epoch_{epoch+1}.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # Save model checkpoint and generate samples\n",
        "        if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'loss': avg_loss,\n",
        "            }, f\"model_checkpoints/diffusion_epoch_{epoch+1}.pt\")\n",
        "\n",
        "            # Generate and save samples\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                samples = diffusion.sample(model, n_samples=4)\n",
        "\n",
        "                # Save the generated samples\n",
        "                fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
        "                axes = axes.flatten()\n",
        "\n",
        "                for i, sample in enumerate(samples):\n",
        "                    # Convert from [-1, 1] to [0, 1] for display\n",
        "                    sample_img = (sample.cpu().squeeze() + 1) / 2\n",
        "                    axes[i].imshow(sample_img, cmap='viridis')\n",
        "                    axes[i].axis('off')\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(f\"generated_samples/samples_epoch_{epoch+1}.png\")\n",
        "                plt.close()\n",
        "\n",
        "    # Calculate FID score after training\n",
        "    model.eval()\n",
        "\n",
        "    # Get a batch of real images\n",
        "    real_images = []\n",
        "    try:\n",
        "        for batch in dataloader:\n",
        "            real_images.append(batch)\n",
        "            if len(real_images) * batch.shape[0] >= 50:  # Reduced to 50 images for speed\n",
        "                break\n",
        "        real_images = torch.cat(real_images, dim=0)[:50].to(device)\n",
        "\n",
        "        # Generate fake images\n",
        "        fake_images = diffusion.sample(model, n_samples=min(50, len(real_images)))\n",
        "\n",
        "        # Calculate FID score\n",
        "        fid_score = calculate_fid(real_images, fake_images)\n",
        "        print(f\"Final FID Score: {fid_score:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in final evaluation: {e}\")\n",
        "        fid_score = float('inf')\n",
        "\n",
        "    return model, fid_score\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-01T10:02:53.557857Z",
          "iopub.execute_input": "2025-04-01T10:02:53.558141Z",
          "iopub.status.idle": "2025-04-01T10:02:53.570119Z",
          "shell.execute_reply.started": "2025-04-01T10:02:53.558119Z",
          "shell.execute_reply": "2025-04-01T10:02:53.569239Z"
        },
        "id": "BU_Zqnu0afHG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main Function**"
      ],
      "metadata": {
        "id": "Dg8A-69ZafHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function\n",
        "def main():\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "    # Define data directory\n",
        "    data_dir = \"/kaggle/input/diffusion/Samples\"\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    dataset = LensingDataset(data_dir)\n",
        "\n",
        "    # Check if dataset is empty\n",
        "    if len(dataset) == 0:\n",
        "        print(f\"No .npy files found in {data_dir}\")\n",
        "        return\n",
        "    else:\n",
        "        print(f\"Found {len(dataset)} samples in the dataset\")\n",
        "\n",
        "    # Create dataloader with fewer workers for Kaggle\n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "    # Create model, diffusion process, and optimizer\n",
        "    model = UNet(in_channels=1, out_channels=1, time_dim=256)\n",
        "    diffusion = DiffusionModel()\n",
        "    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # Train the model\n",
        "    trained_model, fid_score = train(model, diffusion, dataloader, optimizer)\n",
        "\n",
        "    print(f\"Training complete. Final FID Score: {fid_score:.4f}\")\n",
        "\n",
        "    # Generate final samples\n",
        "    os.makedirs(\"final_samples\", exist_ok=True)\n",
        "    trained_model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        samples = diffusion.sample(trained_model, n_samples=16)\n",
        "\n",
        "        # Save the generated samples\n",
        "        fig, axes = plt.subplots(4, 4, figsize=(15, 15))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        for i, sample in enumerate(samples):\n",
        "            # Convert from [-1, 1] to [0, 1] for display\n",
        "            sample_img = (sample.cpu().squeeze() + 1) / 2\n",
        "            axes[i].imshow(sample_img, cmap='viridis')\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"final_samples/final_samples.png\")\n",
        "        plt.close()\n",
        "\n",
        "    print(\"Generated final samples saved to 'final_samples/final_samples.png'\")\n",
        "\n",
        "\n",
        "# Entry point\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-01T10:03:01.708941Z",
          "iopub.execute_input": "2025-04-01T10:03:01.709228Z",
          "iopub.status.idle": "2025-04-01T12:44:13.722034Z",
          "shell.execute_reply.started": "2025-04-01T10:03:01.709207Z",
          "shell.execute_reply": "2025-04-01T12:44:13.721207Z"
        },
        "id": "FBNYAMxqafHG",
        "outputId": "58c93e63-c89a-40e3-9abf-696b1aad4981"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Using device: cuda\nFound 10000 samples in the dataset\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 1/100: 100%|██████████| 313/313 [01:32<00:00,  3.40it/s, loss=0.0442] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/100 - Average Loss: 0.0347 - LR: 0.000200\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.0384] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 2/100 - Average Loss: 0.0110 - LR: 0.000200\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.00211]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 3/100 - Average Loss: 0.0087 - LR: 0.000200\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 4/100: 100%|██████████| 313/313 [01:32<00:00,  3.40it/s, loss=0.00338]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 4/100 - Average Loss: 0.0072 - LR: 0.000199\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 5/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.0174] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 5/100 - Average Loss: 0.0065 - LR: 0.000199\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 6/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.0128] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 6/100 - Average Loss: 0.0066 - LR: 0.000198\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 7/100: 100%|██████████| 313/313 [01:32<00:00,  3.40it/s, loss=0.00135] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 7/100 - Average Loss: 0.0067 - LR: 0.000198\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 8/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.0101]  \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 8/100 - Average Loss: 0.0058 - LR: 0.000197\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 9/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.0159]  \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 9/100 - Average Loss: 0.0063 - LR: 0.000196\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 10/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.00135] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 10/100 - Average Loss: 0.0059 - LR: 0.000195\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Sampling: 100%|██████████| 1000/1000 [00:23<00:00, 42.13it/s]\nEpoch 11/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00116] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 11/100 - Average Loss: 0.0062 - LR: 0.000194\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 12/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.0025]  \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 12/100 - Average Loss: 0.0053 - LR: 0.000193\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 13/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.00206] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 13/100 - Average Loss: 0.0061 - LR: 0.000192\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 14/100: 100%|██████████| 313/313 [01:32<00:00,  3.40it/s, loss=0.0097]  \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 14/100 - Average Loss: 0.0057 - LR: 0.000190\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 15/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.00564] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 15/100 - Average Loss: 0.0054 - LR: 0.000189\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 16/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.0395]  \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 16/100 - Average Loss: 0.0053 - LR: 0.000188\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 17/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00105] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 17/100 - Average Loss: 0.0060 - LR: 0.000186\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 18/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00232] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 18/100 - Average Loss: 0.0053 - LR: 0.000184\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 19/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00303] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 19/100 - Average Loss: 0.0051 - LR: 0.000183\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 20/100: 100%|██████████| 313/313 [01:32<00:00,  3.40it/s, loss=0.00202] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 20/100 - Average Loss: 0.0048 - LR: 0.000181\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Sampling: 100%|██████████| 1000/1000 [00:23<00:00, 42.11it/s]\nEpoch 21/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00158] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 21/100 - Average Loss: 0.0056 - LR: 0.000179\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 22/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.000649]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 22/100 - Average Loss: 0.0055 - LR: 0.000177\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 23/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.000481]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 23/100 - Average Loss: 0.0053 - LR: 0.000175\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 24/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.000871]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 24/100 - Average Loss: 0.0053 - LR: 0.000173\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 25/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00257] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 25/100 - Average Loss: 0.0052 - LR: 0.000171\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 26/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.0322]  \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 26/100 - Average Loss: 0.0051 - LR: 0.000168\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 27/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.00109] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 27/100 - Average Loss: 0.0053 - LR: 0.000166\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 28/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.00396] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 28/100 - Average Loss: 0.0050 - LR: 0.000164\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 29/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.00115] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 29/100 - Average Loss: 0.0058 - LR: 0.000161\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 30/100: 100%|██████████| 313/313 [01:32<00:00,  3.40it/s, loss=0.0017]  \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 30/100 - Average Loss: 0.0048 - LR: 0.000159\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Sampling: 100%|██████████| 1000/1000 [00:23<00:00, 42.11it/s]\nEpoch 31/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00135] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 31/100 - Average Loss: 0.0047 - LR: 0.000156\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 32/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.000695]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 32/100 - Average Loss: 0.0045 - LR: 0.000154\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 33/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00108] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 33/100 - Average Loss: 0.0049 - LR: 0.000151\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 34/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.0105]  \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 34/100 - Average Loss: 0.0047 - LR: 0.000148\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 35/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00256] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 35/100 - Average Loss: 0.0049 - LR: 0.000145\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 36/100: 100%|██████████| 313/313 [01:32<00:00,  3.40it/s, loss=0.00226] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 36/100 - Average Loss: 0.0054 - LR: 0.000143\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 37/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.0191]  \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 37/100 - Average Loss: 0.0054 - LR: 0.000140\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 38/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.000703]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 38/100 - Average Loss: 0.0053 - LR: 0.000137\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 39/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00476] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 39/100 - Average Loss: 0.0048 - LR: 0.000134\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 40/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.000552]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 40/100 - Average Loss: 0.0055 - LR: 0.000131\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Sampling: 100%|██████████| 1000/1000 [00:23<00:00, 42.13it/s]\nEpoch 41/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.000931]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 41/100 - Average Loss: 0.0054 - LR: 0.000128\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 42/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.000562]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 42/100 - Average Loss: 0.0050 - LR: 0.000125\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 43/100: 100%|██████████| 313/313 [01:32<00:00,  3.40it/s, loss=0.000934]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 43/100 - Average Loss: 0.0046 - LR: 0.000122\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 44/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00147] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 44/100 - Average Loss: 0.0045 - LR: 0.000119\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 45/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.000891]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 45/100 - Average Loss: 0.0048 - LR: 0.000116\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 46/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.00417] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 46/100 - Average Loss: 0.0046 - LR: 0.000113\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 47/100: 100%|██████████| 313/313 [01:32<00:00,  3.40it/s, loss=0.00162] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 47/100 - Average Loss: 0.0045 - LR: 0.000109\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 48/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.0201]  \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 48/100 - Average Loss: 0.0048 - LR: 0.000106\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 49/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00274] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 49/100 - Average Loss: 0.0051 - LR: 0.000103\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 50/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.000877]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 50/100 - Average Loss: 0.0041 - LR: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Sampling: 100%|██████████| 1000/1000 [00:23<00:00, 42.13it/s]\nEpoch 51/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00201] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 51/100 - Average Loss: 0.0049 - LR: 0.000097\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 52/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00373] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 52/100 - Average Loss: 0.0046 - LR: 0.000094\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 53/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00944] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 53/100 - Average Loss: 0.0047 - LR: 0.000091\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 54/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00212] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 54/100 - Average Loss: 0.0049 - LR: 0.000087\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 55/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00406] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 55/100 - Average Loss: 0.0045 - LR: 0.000084\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 56/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00618] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 56/100 - Average Loss: 0.0055 - LR: 0.000081\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 57/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.000439]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 57/100 - Average Loss: 0.0044 - LR: 0.000078\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 58/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00345] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 58/100 - Average Loss: 0.0045 - LR: 0.000075\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 59/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.000301]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 59/100 - Average Loss: 0.0048 - LR: 0.000072\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 60/100: 100%|██████████| 313/313 [01:32<00:00,  3.40it/s, loss=0.000432]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 60/100 - Average Loss: 0.0047 - LR: 0.000069\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Sampling: 100%|██████████| 1000/1000 [00:23<00:00, 42.12it/s]\nEpoch 61/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.000414]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 61/100 - Average Loss: 0.0052 - LR: 0.000066\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 62/100: 100%|██████████| 313/313 [01:32<00:00,  3.40it/s, loss=0.000978]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 62/100 - Average Loss: 0.0049 - LR: 0.000063\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 63/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.0141]  \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 63/100 - Average Loss: 0.0040 - LR: 0.000060\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 64/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.00326] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 64/100 - Average Loss: 0.0047 - LR: 0.000057\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 65/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.00383] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 65/100 - Average Loss: 0.0050 - LR: 0.000055\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 66/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.000468]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 66/100 - Average Loss: 0.0044 - LR: 0.000052\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 67/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00201] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 67/100 - Average Loss: 0.0045 - LR: 0.000049\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 68/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.000338]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 68/100 - Average Loss: 0.0047 - LR: 0.000046\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 69/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.00528] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 69/100 - Average Loss: 0.0043 - LR: 0.000044\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 70/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.00868] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 70/100 - Average Loss: 0.0046 - LR: 0.000041\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Sampling: 100%|██████████| 1000/1000 [00:23<00:00, 42.12it/s]\nEpoch 71/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.000385]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 71/100 - Average Loss: 0.0042 - LR: 0.000039\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 72/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.001]   \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 72/100 - Average Loss: 0.0040 - LR: 0.000036\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 73/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00307] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 73/100 - Average Loss: 0.0051 - LR: 0.000034\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 74/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00339] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 74/100 - Average Loss: 0.0046 - LR: 0.000032\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 75/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.000182]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 75/100 - Average Loss: 0.0047 - LR: 0.000029\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 76/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.00502] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 76/100 - Average Loss: 0.0045 - LR: 0.000027\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 77/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00616] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 77/100 - Average Loss: 0.0046 - LR: 0.000025\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 78/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00107] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 78/100 - Average Loss: 0.0042 - LR: 0.000023\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 79/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.0168]  \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 79/100 - Average Loss: 0.0048 - LR: 0.000021\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 80/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00244] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 80/100 - Average Loss: 0.0049 - LR: 0.000019\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Sampling: 100%|██████████| 1000/1000 [00:23<00:00, 42.12it/s]\nEpoch 81/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00988] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 81/100 - Average Loss: 0.0046 - LR: 0.000017\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 82/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00868] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 82/100 - Average Loss: 0.0044 - LR: 0.000016\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 83/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.000615]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 83/100 - Average Loss: 0.0045 - LR: 0.000014\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 84/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.0101]  \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 84/100 - Average Loss: 0.0048 - LR: 0.000012\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 85/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.0105]  \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 85/100 - Average Loss: 0.0048 - LR: 0.000011\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 86/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00254] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 86/100 - Average Loss: 0.0042 - LR: 0.000010\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 87/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00112] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 87/100 - Average Loss: 0.0043 - LR: 0.000008\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 88/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.000485]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 88/100 - Average Loss: 0.0046 - LR: 0.000007\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 89/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.0036]  \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 89/100 - Average Loss: 0.0044 - LR: 0.000006\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 90/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.0283]  \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 90/100 - Average Loss: 0.0041 - LR: 0.000005\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Sampling: 100%|██████████| 1000/1000 [00:23<00:00, 42.12it/s]\nEpoch 91/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00219] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 91/100 - Average Loss: 0.0044 - LR: 0.000004\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 92/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.0112]  \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 92/100 - Average Loss: 0.0043 - LR: 0.000003\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 93/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.00225] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 93/100 - Average Loss: 0.0048 - LR: 0.000002\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 94/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00309] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 94/100 - Average Loss: 0.0044 - LR: 0.000002\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 95/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00164] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 95/100 - Average Loss: 0.0046 - LR: 0.000001\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 96/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.000701]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 96/100 - Average Loss: 0.0044 - LR: 0.000001\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 97/100: 100%|██████████| 313/313 [01:31<00:00,  3.40it/s, loss=0.00111] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 97/100 - Average Loss: 0.0045 - LR: 0.000000\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 98/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.0159]  \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 98/100 - Average Loss: 0.0045 - LR: 0.000000\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 99/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00445] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 99/100 - Average Loss: 0.0048 - LR: 0.000000\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 100/100: 100%|██████████| 313/313 [01:31<00:00,  3.41it/s, loss=0.00587] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 100/100 - Average Loss: 0.0048 - LR: 0.000000\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Sampling: 100%|██████████| 1000/1000 [00:23<00:00, 42.12it/s]\nSampling: 100%|██████████| 1000/1000 [02:39<00:00,  6.25it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Feature shapes - Real: (50, 1000), Fake: (50, 1000)\nCovariance shapes - Real: (1000, 1000), Fake: (1000, 1000)\nFinal FID Score: 34.3615\nTraining complete. Final FID Score: 34.3615\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Sampling: 100%|██████████| 1000/1000 [00:56<00:00, 17.73it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Generated final samples saved to 'final_samples/final_samples.png'\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "_URRYTNHafHL"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}